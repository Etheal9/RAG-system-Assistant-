
## The Science of Chunking: Why Your RAG System’s Success Lives and Dies Here

When you’re starting with RAG, chunking is the foundation of your entire system. Your embeddings can only be as good as the text you embed. Your retrieval can only be as precise as the chunks you retrieve. Your generation can only be as accurate as the context you provide.

This article will walk you through the science of chunking for production RAG systems. We’ll start with why chunking matters so much, explore the evolution from naive to state-of-the-art strategies, and look into choosing the right approach for your use case. By the end, you’ll understand not just how to chunk, but why each strategy works and when to use it.

The Chunking Goldilocks Problem
Before we dive into strategies, let’s understand why chunking is so challenging. You’re facing what I call the Chunking Goldilocks Problem: chunks that are too big create noisy context, chunks that are too small lose critical information, and finding the “just right” size depends entirely on your data and use case.

When chunks are too large, several issues emerge. Your LLM has to wade through a thousand-word chunk to find the one sentence that answers the question. Your embeddings become diluted, representing too many concepts at once, which hurts retrieval precision. You hit token limits faster and pay more for processing. Most importantly, when you retrieve a large chunk, you’re often bringing in irrelevant information alongside what you actually need.

On the flip side, when chunks are too small, you get context-deficient snippets. A single sentence might be technically accurate but meaningless without the surrounding paragraphs. Key relationships between concepts get lost. Tables get separated from their headers. Code examples break mid-function. The LLM receives fragmented information and struggles to generate coherent, accurate responses.

The challenge is that there’s no universal chunk size that works for everything. A technical document with dense information might need larger chunks to preserve context. A FAQ dataset might work better with small, focused chunks. Financial reports with tables need chunks that respect document structure. The optimal strategy depends on your content type, your query patterns, and your accuracy requirements.

Understanding the Fundamentals
Let’s establish what we’re actually trying to achieve with chunking. At its core, chunking is about creating retrieval units that maximize the probability of finding relevant information while minimizing noise. Each chunk becomes an independent entity in your vector database, with its own embedding that captures its semantic meaning.

When a user asks a question, your system converts that question into an embedding and searches for the closest matching chunks. This is where chunk quality becomes critical. If a chunk contains a mix of unrelated concepts, its embedding will be a muddy average of those concepts. If a chunk is too focused, it might miss queries that need broader context. The art of chunking is creating units that are semantically coherent, contextually complete, and optimally sized for both embeddings and generation.

There’s also a practical consideration that affects every chunking decision: the token limits of your models. Embedding models typically max out at around 8,192 tokens, though some newer models support more. Your chunks need to fit within this limit. More importantly, when you retrieve multiple chunks to provide context to your LLM, the combined length of all chunks plus your prompt needs to fit within the LLM’s context window.

This means you’re not just optimizing for a single chunk size, you’re optimizing for the k chunks you’ll retrieve. If you retrieve the top 5 chunks and each is 1,000 tokens, you’re using 5,000 tokens just for context. Add your system prompt, the user’s question, and any conversation history, and you can see how quickly you approach context limits.

The Evolution of Chunking Strategies
Let’s walk through the evolution of chunking strategies, from the simplest baseline to the cutting-edge approaches being used in production systems today. Understanding this progression will help you make informed decisions about which strategy fits your needs.

Fixed-Size Chunking: The Naive Baseline
The simplest approach is fixed-size chunking. You pick a number of characters or tokens, split the text when you hit that limit, and optionally add some overlap between chunks. For example, you might create 512-token chunks with a 50-token overlap, ensuring that information near chunk boundaries appears in multiple chunks.

This method is dead simple to implement. It’s fast, predictable, and requires zero understanding of the content. For these reasons, it’s what most RAG tutorials start with. Here’s a basic implementation:

def fixed_size_chunk(text, chunk_size=512, overlap=50):
    # Split text into fixed-size chunks with overlap 
    chunks = []
    start = 0
    
    while start < len(text):
        end = start + chunk_size
        chunk = text[start:end]
        chunks.append(chunk)
        start += chunk_size - overlap  # Move forward, accounting for overlap
    
    return chunks
The problem with fixed-size chunking is that it’s completely blind to content structure. It will split sentences mid-way. It will separate a header from its content. It will cut a paragraph at an arbitrary point. This leads to semantic fragmentation, where concepts that should stay together get split across chunks, hurting both embedding quality and retrieval accuracy.

That said, fixed-size chunking isn’t always wrong. For highly uniform content like news articles or social media posts where each piece is roughly the same length and structure, fixed-size chunking with appropriate overlap can work reasonably well. The key is understanding its limitations and only using it when those limitations don’t hurt your use case.

Recursive Chunking: Respecting Structure
Recursive chunking represents a significant step up from fixed-size splitting. Instead of blindly cutting at character or token limits, it tries to split text at natural boundaries using a hierarchy of separators. The algorithm attempts to split on high-level separators first like double newlines between paragraphs, then falls back to lower-level separators like single newlines, then periods, and finally spaces if necessary.

This approach respects document structure while still maintaining control over chunk sizes. A paragraph stays together if it fits within your chunk size limit. If it doesn’t, the algorithm breaks it at sentence boundaries. If individual sentences are too long, it breaks at spaces. This creates chunks that are much more semantically coherent than fixed-size splitting.

Here’s how LangChain implements this, which has become the standard:

from langchain_text_splitters import RecursiveCharacterTextSplitter
# Default separators: ["\n\n", "\n", " ", ""]
splitter = RecursiveCharacterTextSplitter(
    chunk_size=512,
    chunk_overlap=50,
    length_function=len,  # Can also use token counting
    separators=["\n\n", "\n", ". ", "? ", "! ", " ", ""]
)
chunks = splitter.split_text(your_text)
The beauty of recursive chunking is its flexibility. You can customize the separator hierarchy for your content type. For code, you might prioritize function boundaries and class definitions. For markdown, you might use headers as primary separators. For dialogue, you might split on speaker changes.

Research from Chroma shows that recursive chunking with 400-token chunks and 10–20% overlap achieves 88–89% recall using OpenAI’s text-embedding-3-large model. This makes it the default choice for about 80% of RAG applications, according to industry surveys. It balances simplicity with structure awareness, making it a solid baseline for most use cases.

The limitation of recursive chunking is that it’s still primarily structure-based rather than meaning-based. It doesn’t understand that two paragraphs discuss the same concept and should stay together, or that a paragraph is a tangent that could be separated. For cases where semantic coherence matters more than structural boundaries, you need something more sophisticated.

Semantic Chunking: Following Meaning
Semantic chunking represents a paradigm shift. Instead of looking for structural markers, it analyzes the actual meaning of the text and creates chunks where topics change. The algorithm computes embeddings for sentences or small text units, measures similarity between consecutive units, and creates chunk boundaries where similarity drops below a threshold.

This approach creates chunks that are semantically coherent. All sentences within a chunk discuss related concepts, and chunk boundaries occur at natural topic shifts. This dramatically improves both embedding quality and retrieval accuracy, because each chunk represents a cohesive idea rather than an arbitrary text segment.

The implementation typically works like this: you split the text into sentences, compute embeddings for each sentence, then measure cosine similarity between consecutive sentence embeddings. When the similarity drops below a threshold (typically 0.5 to 0.7), you create a new chunk. You can also use a sliding window approach, comparing groups of sentences rather than individual ones.

from langchain_experimental.text_splitter import SemanticChunker
from langchain_openai import OpenAIEmbeddings
# Using semantic similarity to determine chunk boundaries
text_splitter = SemanticChunker(
    OpenAIEmbeddings(),
    breakpoint_threshold_type="percentile",  # or "standard_deviation", "interquartile"
    breakpoint_threshold_amount=50  # Adjust sensitivity
)
chunks = text_splitter.split_text(your_text)
Research from Antematter analyzing multiple chunking strategies found that semantic chunking consistently outperformed other methods in retrieval quality. Chroma’s research showed LLM-based semantic chunking achieved 91.9% recall, the highest among all strategies tested. The improvement is particularly noticeable for complex documents where topics interweave, like research papers or technical documentation.

The trade-off is computational cost. Semantic chunking requires computing embeddings for every sentence during document processing, which is expensive for large document collections. It’s also slower than structure-based methods. For ongoing document ingestion at scale, this can add significant infrastructure costs. The strategy works best when you’re processing high-value documents where retrieval quality is critical, like legal contracts, medical literature, or critical enterprise knowledge bases.

Page-Level Chunking: When Structure Matters
Here’s a surprising finding from NVIDIA’s 2024 benchmark study: across five diverse datasets, page-level chunking achieved the highest average accuracy (0.648) with the lowest standard deviation (0.107). This means it performed consistently well across different document types, outperforming both token-based and section-level approaches.


Page-level chunking is exactly what it sounds like: each page of a document becomes a single chunk. This strategy works exceptionally well for structured documents like financial reports, legal filings, and research papers, where authors organize information by page and pages contain complete, self-contained units of meaning.

The reason this works so well is subtle but important. Documents like 10-K filings or earnings reports are deliberately structured so each page or section presents complete information. Tables appear with their context. Discussions flow logically within page boundaries. When you respect these natural boundaries, you preserve the semantic completeness that authors intended.

The implementation is straightforward when working with PDFs:

import PyPDF2
def page_level_chunk(pdf_path):
    chunks = []
    with open(pdf_path, 'rb') as file:
        pdf_reader = PyPDF2.PdfReader(file)
        for page_num in range(len(pdf_reader.pages)):
            page = pdf_reader.pages[page_num]
            text = page.extract_text()
            chunks.append({
                'content': text,
                'metadata': {
                    'page': page_num + 1,
                    'source': pdf_path
                }
            })
    return chunks
However, page-level chunking has an important caveat: it only works when pagination is meaningful. If your PDFs are just text exports with arbitrary page breaks, or if pages vary wildly in content density, page-level chunking won’t help and might hurt. NVIDIA’s benchmark tested documents where page boundaries had semantic meaning — financial reports, not raw text dumps.

The strategy also creates variability in chunk sizes. Some pages might have 100 tokens, others 2,000. This can affect embedding quality and retrieval consistency. You might need to combine very short pages or split very long ones to maintain reasonable chunk sizes. But when applied to the right document types, page-level chunking’s performance is hard to beat.

Hierarchical Chunking: The Parent-Child Strategy
One of the most effective advanced strategies is hierarchical or parent-child chunking. This approach recognizes a fundamental tension in chunking: you want small chunks for precise embedding and matching, but you need large chunks for complete context when generating answers.

The solution is to create both. You chunk your documents into small “child” chunks (100–500 tokens) that enable precise retrieval, but you also maintain larger “parent” chunks (500–2000 tokens) that contain those children. When a query matches a child chunk, you return its parent to the LLM, giving you both precision and context.

This architecture is particularly powerful because it decouples the retrieval and generation concerns. Your embeddings are focused and specific, improving search accuracy. But your LLM gets rich, complete context, improving generation quality. You get the best of both worlds.

from langchain.retrievers import ParentDocumentRetriever
from langchain_community.vectorstores import FAISS
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.storage import InMemoryStore
# Create parent chunks (larger)
parent_splitter = RecursiveCharacterTextSplitter(
    chunk_size=2000,
    chunk_overlap=200
)
# Create child chunks (smaller)
child_splitter = RecursiveCharacterTextSplitter(
    chunk_size=400,
    chunk_overlap=40
)
# Parent Document Retriever handles the relationship
retriever = ParentDocumentRetriever(
    vectorstore=vectorstore,
    docstore=docstore,
    parent_splitter=parent_splitter,
    child_splitter=child_splitter,
)
The implementation requires tracking the relationship between parent and child chunks, typically through metadata that maps each child to its parent. When you retrieve relevant children, you look up their parent IDs and return those larger chunks to the LLM. This adds some complexity to your ingestion pipeline and requires additional storage, but the retrieval quality improvements are substantial.

Hierarchical chunking works especially well for documents with clear structure like technical documentation, research papers, or educational content. The parent-child relationships can mirror the document’s natural hierarchy — chapters contain sections, sections contain paragraphs. This structural awareness improves both retrieval and generation.

Context-Enriched Chunking: Adding What’s Missing
Another powerful technique is context-enriched chunking, where you augment each chunk with contextual information from the surrounding text or the broader document. The insight is that chunks are often ambiguous when viewed in isolation. Adding a summary of the document or the surrounding context helps both embedding and retrieval.

One approach is sentence window retrieval. You create small chunks for precise matching, but when you retrieve a chunk, you expand it by including the sentences before and after it. This gives the LLM the focused chunk plus its immediate context, reducing the context loss that comes from tight chunking.

Another approach, popularized by Anthropic, is contextual retrieval. For each chunk, you use an LLM to generate a brief context statement that explains what the chunk discusses in relation to the whole document. This context is prepended to the chunk before embedding, creating richer, more informative embeddings.

# Simple contextual retrieval example
def add_context_to_chunk(chunk, document, model):
    prompt = f"""
    Document context: {document[:500]}...
    
    Chunk to contextualize: {chunk}
    
    Provide a brief (1-2 sentences) context for this chunk 
    explaining what it discusses and how it relates to the document.
    """
    context = model.generate(prompt)
    return f"{context}\n\n{chunk}"
Anthropic’s research showed that contextual retrieval reduced failed retrievals by 35% compared to baseline RAG, and by 49% when combined with hybrid search (BM25 + vector). The improvement comes from giving the embedding model more signals about what each chunk represents.

The trade-off is cost and latency during ingestion. You’re making an LLM call for every chunk to generate context, which adds up for large document collections. This makes contextual retrieval most suitable for high-value documents or when retrieval accuracy is critical enough to justify the cost. For real-time ingestion of high-volume content, simpler strategies like sentence window retrieval might be more practical.

Choosing Your Chunking Strategy: A Decision Framework
Now that we’ve covered the major strategies, let’s talk about how to choose. The right strategy depends on several factors: your content type, your query patterns, your accuracy requirements, and your resource constraints.

Start with your content type. For structured documents with clear sections like technical documentation or business reports, recursive chunking with custom separators works well. For documents where pagination matters like financial filings or academic papers, consider page-level chunking. For dense, topic-rich content like research literature or legal texts, semantic chunking or hierarchical approaches pay off. For code repositories, you need code-aware chunking that respects function boundaries.

Consider your query patterns. If users ask specific, factual questions, smaller chunks with high precision work well. If users ask analytical questions requiring broader context, larger chunks or hierarchical retrieval make sense. If queries span multiple topics, parent-child chunking helps by letting you retrieve multiple relevant children and assemble their contexts.

Think about your accuracy requirements. For customer-facing systems where wrong answers create real problems, invest in higher-quality chunking like semantic or contextual approaches. For internal tools where some imprecision is acceptable, simpler strategies suffice. For compliance or safety-critical applications, the extra cost of advanced chunking is easily justified.

Finally, account for your resource constraints. If you’re processing millions of documents or have real-time ingestion requirements, the computational cost of semantic or contextual chunking might be prohibitive. Start with recursive chunking and upgrade to more sophisticated strategies only where needed. You can even use different strategies for different document types, routing PDFs to page-level chunking while using recursive chunking for web content.

Here’s a practical recommendation for getting started: begin with recursive chunking at 400–512 tokens with 10–20% overlap. This gives you a solid baseline that works for most content types. Measure your retrieval quality using metrics like context precision and recall (we’ll cover this in the next article). If your metrics show problems, experiment with semantic chunking for complex documents or hierarchical chunking for better context preservation.

Practical Implementation Considerations
Beyond choosing a strategy, several practical decisions affect your chunking quality. Let’s walk through the key considerations you’ll face in production.

First is the overlap between chunks. Overlap ensures that information near chunk boundaries appears in multiple chunks, reducing the chance that splitting breaks important context. The industry standard is 10–20% overlap, meaning if you have 500-token chunks, you’d overlap 50–100 tokens. Research shows this range works well across most document types, though you can tune it based on your specific needs.

Second is metadata enrichment. When you create chunks, attach metadata that helps with retrieval and generation. At minimum, include the source document, the chunk’s position within that document, and any structural information like section headers. For hierarchical chunking, include parent-child relationships. For documents with tables or code, mark those chunks appropriately so you can handle them specially.

Third is handling special content types. Tables need special treatment — either keep them whole as single chunks, or if they’re large, split them while preserving headers and keeping related rows together. Code should be chunked at logical boundaries like function definitions, preserving complete, executable units. Images and charts in documents should be processed separately, potentially using multimodal embeddings or generating textual descriptions to include with surrounding text.

Fourth is the relationship between chunk size and your embedding model. Different embedding models have different optimal input sizes. Some models perform better with shorter inputs around 256 tokens, others with longer inputs up to 512 or more. Check your embedding model’s documentation and benchmark to find the sweet spot for your model.

Finally, consider chunk size in relation to your retrieval parameters. If you always retrieve the top 5 chunks and each is 1,000 tokens, you’re consistently using 5,000 tokens of context. This might push against your LLM’s context limits. You might want to reduce chunk size, retrieve fewer chunks, or use hierarchical chunking to balance precision and context.

Looking Ahead
Chunking is evolving rapidly as RAG systems mature. We’re seeing the emergence of multimodal chunking that handles images, tables, and text together, preserving the relationships between visual and textual elements. We’re seeing LLM-based agentic chunking where an AI analyzes each document and decides the optimal chunking strategy on the fly. We’re seeing late chunking techniques that embed entire documents first, then create chunk-level embeddings from the full-document representation.

The fundamental principle remains: chunking is about creating retrieval units that maximize relevant information while minimizing noise. Whether you use a simple recursive splitter or a sophisticated agentic approach, your goal is to give your retrieval system the best possible chance of finding what users need, and give your generation system the best possible context for creating accurate answers.


